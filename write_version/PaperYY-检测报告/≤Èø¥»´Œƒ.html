<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>PaperYY英文检测报告</title>
    <script src="js/jquery.1.7.2.js"></script>
    <link rel="stylesheet" href="css/css.css">
    <script src="js/js.js"></script>
    <script type="text/javascript" src="js/en_report.js"></script>
    <style type="text/css">
      .ownsentence em{
        color: red;
      }
      .similar em{
        color: #39ab1a;
      }
      .
    </style>
    <script type="text/javascript">
        var reg = new RegExp("(^|&)data=([^&]*)(&|$)", "i");

        function getQueryString() {
            var search = location.search;
            var param = search.substr(1).match(reg);
            if (param == null){
            $(".hd-nav").show();
            return;
            }
            var encode = param[2];
            var decode = decodeURIComponent(encode);
            var data = JSON.parse(decode);
            var urlMap = data.UrlMap;
            $(".menu a").each(function () {
                var key = $(this).attr("href");
                if(urlMap[key]) {
                    var url;
                    if (urlMap[key].indexOf("?") > -1) {
                        url = urlMap[key] + "&data=" + encode;
                    } else {
                        url = urlMap[key] + "?data=" + encode;
                    }
                    $(this).attr("href", url);
                }
            });
            $(".hd-nav").show();
        }
        $(document).ready(function () {
            getQueryString();
        });
    </script>
</head>

<body>
    <!-- header -->
    <div class="header">
        <!-- 20190307 s -->
        <div class="hd-top px-30">
            <a class="head-logo" href=""><img src="img/logo.png" alt=""></a>
            <div class="title">
                <h1>paper01</h1>
                <div class="data">
                    <span>论文作者：</span>
                    <span>论文字数：38,162</span>
                    <span>提交时间：2023-01-18 11:15:30</span>
                    <span>报告编号： YY202301181115284433 <a href="https://www.paperyy.com/reportcheck" target="_blank">查询真伪</a></span>
                </div>
            </div>
            <div class="head-like">
                <div class="inner">
                    <p>相似度</p><b>12.1%</b>
                </div>
            </div>
        </div>
        <div class="hd-nav" >
            <span class="menu" >
                                        <a href="使用帮助.html" >使用帮助<i class="icon-sj"></i></a> 
                                        <a class="on"  href="#">查看全文<i class="icon-sj"></i></a>
                                        <a href="综合报告.html" >综合报告<i class="icon-sj"></i></a> 
                                        <a href="详细报告.html" >详细报告<i class="icon-sj"></i></a> 
                <a href="https://report.paperyy.com/20230118/1-c7693a7a-45a7-4e61-9a7f-0f6191a43775/report.pdf" style="display: none;">PDF报告 <i class="icon-sj"></i></a>
                <a href="#" style="display: none;">在线改重 <i class="icon-sj"></i></a>
            </span>
            <em class="line"></em>
            <span class="tool c-fff">
                <a class="ver" href="https://www.paperyy.com/member_new/thesis/post.aspx?en=true"><i class="icon-search"></i><em>再次检测</em></a>
                <a href="https://enreport.paperyy.com/20230118/1-c7693a7a-45a7-4e61-9a7f-0f6191a43775/report.zip"><i class="icon-down"></i><em>下载报告</em></a>
                <a class="head-phone-wrap" href="javascript:void(0)" style="display: none;"><i class="icon-phone"></i><em>手机预览</em>
                    <div class="phone-pop">
                        <img width="132" height="132" src="data:image/png;base64,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"/>
                    </div>
                </a>
            </span>
            <em class="line"></em>
            
            
            
        </div>
        <!-- 20190307 e -->
    </div>
<!-- e header -->
<!-- main -->
 <div class="main ">
     <div class=" overflow-auto wrap">
        <div class="artical2">
            <p>
                     <em>Flooding based mobilenet v3 identiﬁes cucumber disease leaves in1
fuzzy scenes2</em>
            </p>
            <p>
                     <em>Liu Yiminga, Wang Zhengleb, Wang Rujiac and *Gao Hongjub 
a</em>
            </p>
            <p>
                     <em>School of Computer Science(National Pilot Software Engineering School),Beijing University of Posts and4</em>
            </p>
            <p>
                     <em>Telecommunications,100876,China;liuyimingbyr@bupt.edu.cn5
b</em>
            </p>
            <p>
                     <em>College of Information and Electrical Engineering, China Agricultural University, Beijing 100083,6</em>
            </p>
            <p>
                     <em>China;wangzhengle@cau.edu.cn;hjgao@cau.edu.cn7
c</em>
            </p>
            <p>
                     <em>School of Information and Communication Engineering,School of Computer Science,Beijing University of Posts and8</em>
            </p>
            <p>
                     <em>Telecommunications,100876,China;</em>
                     <em>WNHwrj@bupt.edu.cn9
A R T I C L E I N F O</em>
            </p>
            <p>
                     <em>Keywords:</em>
            </p>
            <p>
                     <em>Flooding;Lightweight;Mobilenet v3;Cucumber 
disease;Crop question and answer</em>
            </p>
            <p>
                     <em>11 A B S T R A C T2</em>
            </p>
            <p>
                     <em>Domestic cucumber production is declining due to various pathologic diseases, but the technol-14</em>
            </p>
            <p>
                     <em>ogy of plant pathologic detection is not mature and requires high labor costs. In addition, since15</em>
            </p>
            <p>
                     <em>the planting site is usually a high-density scene, most photos taken are shot from various angles,16</em>
            </p>
            <p>
                     <em>and the background is messy, resulting in poor detection reliability.</em>
                     <em> The rise of online question-17</em>
            </p>
            <p>
                     <em>and-answer systems is an inspiration.</em>
                     <em> This paper wants to establish an online QA system. Farm-18</em>
            </p>
            <p>
                     <em>ers can upload cucumber pictures by taking photos, and the system can quickly identify and judge19</em>
            </p>
            <p>
                     <em>with high accuracy.</em>
                     <em> In this paper, the crawler program is used to collect many cucumber leaf20</em>
            </p>
            <p>
                     <em>image data in batches on an agricultural website, and simple preprocessing is carried out. With21</em>
            </p>
            <p>
                     <em>a lightweight and fast MobileNetv3 network structure, it can quickly and accurately complete22</em>
            </p>
            <p>
                     <em>the seven kinds of cucumber leaf disease classiﬁcation.</em>
                     <em> The optimal network model is achieved23</em>
            </p>
            <p>
                     <em>by selecting appropriate parameters, optimizer, and batch capacity through the single variable24</em>
            </p>
            <p>
                     <em>method.</em>
                     <em> In addition, a new training strategy of data set loss -ﬂooding method was introduced25</em>
            </p>
            <p>
                     <em>in this paper, replacing the strategy of ﬂooding after the ﬂooding threshold was reached, which26</em>
            </p>
            <p>
                     <em>ﬁnally achieved 88.3% accuracy.</em>
                     <em> Finally, two public data sets of PlantVillage and apple disease27</em>
            </p>
            <p>
                     <em>were selected for another experiment.</em>
                     <em> The accuracy was up to 99% and 98.1%, respectively,28</em>
            </p>
            <p>
                     <em>which proved the universality of the strategy proposed in this paper.</em>
                     <em> In this paper, the code29</em>
            </p>
            <p>
                     <em>will be open source in https://github.</em>
                     <em>com/YiQuanMarx/Agricultural_Diseases_Dentiﬁcation for30
reference.31</em>
            </p>
            <p>
                     <em>CRediT authorship contribution statement33</em>
            </p>
            <p>
                     <em>Liu Yiming: Responsible for paper experiment conception, data processing, main experiment realization, data34</em>
            </p>
            <p>
                     <em>processing, picture drawing and paper writing and polishing.</em>
                     <em> Wang Zhengle: Participate in the preliminary research35</em>
            </p>
            <p>
                     <em>of the paper, responsible for the partial realization of the paper experiment and the preparation of the paper. Wang36</em>
            </p>
            <p>
                     <em>Rujia: Responsible for data processing, drawing and editing of the paper.</em>
                     <em>*Gao Hongju: Responsible for framing37</em>
            </p>
            <p>
                     <em>the paper, providing data, guiding the writing of the paper, and polishing the paper.38
1. Introduction39</em>
            </p>
            <p>
                     <em>The cucumber, Cucumis sativus, is a widely cultivated creeping vine in the gourd family that usually bears cylindri-40</em>
            </p>
            <p>
                     <em>cal fruits and is used as a vegetable.</em>
                     <em> According to statistics, in 2019, the world produced 88 million tons of cucumbers41</em>
            </p>
            <p>
                     <em>and gherkins, of which China accounted for 80 percent.</em>
                     <em> However, global production of cucumbers is declining as42
</em>
                     <em>various diseases plague them.43</em>
            </p>
            <p>
                     <em>Traditional disease detection methods require manual inspection of diseased leaves through visual cues, which is44</em>
            </p>
            <p>
                     <em>easy to lead to low detection e ciency and poor reliability due to human error.</em>
                     <em> In addition, this labor-intensive task is45</em>
            </p>
            <p>
                     <em>ORCID(s):0000-0002-3006-5919(L. Yiming)</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 1 of 14</em>
            </p>
            <p>
                     <em>complicated and time-consuming by the large area to be detected and the millimeter-scale size of the early symptoms46</em>
            </p>
            <p>
                     <em>to be detected.</em>
                     <em> Compounding the problem is a lack of expertise among farmers, and not enough agricultural experts47</em>
            </p>
            <p>
                     <em>able to spot these diseases also hinder overall harvests.</em>
                     <em> Therefore, if available to farmers tools and techniques exist,48</em>
            </p>
            <p>
                     <em>early detection and classiﬁcation of cucumber diseases can signiﬁcantly alleviate these problems. The emergence of49</em>
            </p>
            <p>
                     <em>an online question-and-answer system provides a suitable treatment method.</em>
                     <em> We can allow farmers to upload photos50</em>
            </p>
            <p>
                     <em>of their cucumber leaves through mobile devices such as mobile phones.</em>
                     <em> After receiving the images, the system will51</em>
            </p>
            <p>
                     <em>process and analyze the pathological results of the cucumbers.</em>
                     <em> The online question-and-answer system has been used52</em>
            </p>
            <p>
                     <em>in all aspects of life, including various shopping software, customer service, online hospital consultation, etc. However,53</em>
            </p>
            <p>
                     <em>the current online question-and-answer system is mainly based on manual identiﬁcation processing, and agricultural54</em>
            </p>
            <p>
                     <em>manual inspection is unsuitable for this situation.</em>
                     <em> Therefore, this paper needs to seek a kind of online question-and-55
</em>
                     <em>answer system without a manual.56</em>
            </p>
            <p>
                     <em>Currently, there are few methods for the pathological analysis of cucumber, including molecular analysis, spectral57</em>
            </p>
            <p>
                     <em>analysis, volatile organic compound analysis, etc.</em>
                     <em> However, these methods are expensive and di cult to apply on a58</em>
            </p>
            <p>
                     <em>commercial scale.</em>
                     <em> Martinelli et al.(2015) In this respect, computer vision has great inherent potential: symptoms of59</em>
            </p>
            <p>
                     <em>crop diseases often cause a feature on plant leaves that can be detected with image-based techniques and appropriate60</em>
            </p>
            <p>
                     <em>strategies.</em>
                     <em> Crop diseases are detected and identiﬁed by analyzing the images’ color, texture, and shape of diseased61
</em>
                     <em>leaves. Benfenati et al.(2021)62</em>
            </p>
            <p>
                     <em>However, many things could still be improved with the current approach.</em>
                     <em> The ﬁrst problem is that existing methods63</em>
            </p>
            <p>
                     <em>need to correctly identify fruit leaf diseases in the Chinese region because all current practices are trained only on the64</em>
            </p>
            <p>
                     <em>PlantVillage dataset, which is based on images from farms in the United States and Switzerland. Fruit diseases also65</em>
            </p>
            <p>
                     <em>di er from other regions due to di erences in leaf shape, variety, and environmental factors.</em>
                     <em> In addition, as about 80%66</em>
            </p>
            <p>
                     <em>of cucumbers are produced in China, there are few widely used data sets for training cucumber leaf detection models.67</em>
            </p>
            <p>
                     <em>Therefore, it is di cult for Chinese farmers to obtain cucumber disease detection technology with high accuracy. We68</em>
            </p>
            <p>
                     <em>urgently need to develop a new data set to detect diseases in cucumber leaves in regions of China so that Chinese farmers69</em>
            </p>
            <p>
                     <em>can identify diseases in cucumbers early, increase their income and boost the country’s economic development.70</em>
            </p>
            <p>
                     <em>Another problem is that professional experts and photographers mostly take the data sets widely used in training71</em>
            </p>
            <p>
                     <em>models.</em>
                     <em> However, most of these photos are taken by farmers who cannot get the perfect shot for analysis, which can72</em>
            </p>
            <p>
                     <em>come in various backgrounds, colors, and sizes.</em>
                     <em> Therefore, it is necessary to train the model in a data set containing73</em>
            </p>
            <p>
                     <em>non-specialized leaf images.</em>
                     <em> The last problem is that most Chinese farmers need high-precision equipment for practical74</em>
            </p>
            <p>
                     <em>applications such as agriculture and generally use mobile terminal devices such as mobile phones. Therefore, we need75</em>
            </p>
            <p>
                     <em>small, low-latency models explicitly tailored for devices with small memory and low computing power. At the same76</em>
            </p>
            <p>
                     <em>time, the results of pathological tests are accurate.77</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 2 of 14</em>
            </p>
            <p>
                     <em>Although previous work achieved high classiﬁcation accuracy on its data set of images of natural cultivation con-78</em>
            </p>
            <p>
                     <em>ditions, several problems still need to be solved.</em>
                     <em> First, deep learning-based disease diagnosis methods require many79</em>
            </p>
            <p>
                     <em>training images.</em>
                     <em> Unlike other general computer vision tasks, labeling disease data sets requires specialized background80</em>
            </p>
            <p>
                     <em>knowledge di cult for farmers to master.</em>
                     <em> Also, to collect perfect images of large data sets, plants must be grown in81</em>
            </p>
            <p>
                     <em>tightly controlled environments, which is labor-intensive and very expensive.</em>
                     <em> Second, overﬁtting problems are partic-82</em>
            </p>
            <p>
                     <em>ularly acute in plant diagnostic tasks because clues related to the disease are often unclear, and other factors, such as83</em>
            </p>
            <p>
                     <em>the image’s background, often signiﬁcantly impact the ﬁnal decision.</em>
                     <em> Not only that, but overﬁtting due to potential84</em>
            </p>
            <p>
                     <em>similarities in the data set often results in a signiﬁcant decline in the accuracy of another data set (for example, images85</em>
            </p>
            <p>
                     <em>from other farms).</em>
                     <em> For example, in cucumber disease diagnosis from wide-angle images, diagnostic performance on86</em>
            </p>
            <p>
                     <em>the same farm showed 86.0% in F1 scores but decreased to 20.7% in di erent farms.</em>
                     <em> To solve the overﬁtting problem,87</em>
            </p>
            <p>
                     <em>Saikawa et al.</em>
                     <em> Saikawa et al.(2019) proposed a method to remove background from the region of interest (RoI) as a88</em>
            </p>
            <p>
                     <em>pre-processing step.</em>
                     <em> The results showed that they could improve accuracy by 12.2%. However, they also point out that89</em>
            </p>
            <p>
                     <em>this approach requires much more expensive masking data, potentially eliminating surrounding information essential90</em>
            </p>
            <p>
                     <em>to a diagnosis. Cap et al.(2020)91</em>
            </p>
            <p>
                     <em>This paper is expected to use a lightweight and fast mobile ETV3 network structure to make it suitable for mobile92</em>
            </p>
            <p>
                     <em>terminal device recognition processing.</em>
                     <em> The optimal network model is achieved by selecting appropriate parameters,93</em>
            </p>
            <p>
                     <em>optimizer, and batch capacity through the single variable method.</em>
                     <em> To further improve its accuracy. With the epoch94</em>
            </p>
            <p>
                     <em>increase, if the loss on the test set reaches a certain threshold and continues training to reduce loss, overﬁtting will95</em>
            </p>
            <p>
                     <em>occur.</em>
                     <em> This paper considers introducing a new training strategy for data set loss to replace the strategy to reduce96</em>
            </p>
            <p>
                     <em>loss after reaching the threshold to solve the overﬁtting problem on the test set.</em>
                     <em> The ﬂooding method is expected to97</em>
            </p>
            <p>
                     <em>improve the situation.</em>
                     <em> Finally, it can achieve high accuracy and better apply to the daily agricultural life of cucumber98
pathological judgment.99</em>
            </p>
            <p>
                     <em>2. Related work100</em>
            </p>
            <p>
                     <em>Recent advances in artiﬁcial intelligence (AI), machine learning (ML), and computer vision (CV) technologies101</em>
            </p>
            <p>
                     <em>have opened up new possibilities, paving the way for the use of data from optical sensors in crop detection by automat-102</em>
            </p>
            <p>
                     <em>ically identifying relevant features.</em>
                     <em> Deep learning is at the heart of intelligent farming through adopting new devices,103</em>
            </p>
            <p>
                     <em>technologies, and algorithms in agriculture 4. Deep learning is widely used to solve complex problems, such as feature104</em>
            </p>
            <p>
                     <em>extraction, transformation, pattern analysis, and image classiﬁcation, which helps signiﬁcantly develop, control, and105</em>
            </p>
            <p>
                     <em>improve agricultural production.106</em>
            </p>
            <p>
                     <em>Over the past few decades, many types of deep learning architectures have been proposed for plant disease classi-107</em>
            </p>
            <p>
                     <em>ﬁcation, resulting in several plant disease diagnosis systems tailored to real cultivation conditions.108</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 3 of 14</em>
            </p>
            <p>
                     <em>In Prasanna Mohanty et al.</em>
                     <em>(2016), Mohanty et al., using a large CNN(Google net and Alexnet), classify 26 diseases109</em>
            </p>
            <p>
                     <em>in 14 crops in 54, PlantVillage Repository Hughes et al.</em>
                     <em>(2015),306 labeled color images of diseased and healthy plant110</em>
            </p>
            <p>
                     <em>leaves formed public data and were trained.</em>
                     <em> The trained model achieved 99.35 percent accuracy on the retention test111</em>
            </p>
            <p>
                     <em>set, demonstrating the feasibility of combining smartphones with computer vision to aid in plant disease diagnosis112
methods.113</em>
            </p>
            <p>
                     <em>In Sladojevic et al.</em>
                     <em>(2016), Sladojevic et al. used the deep learning framework Ca eNet to propose a new method114</em>
            </p>
            <p>
                     <em>to establish a plant disease recognition model.</em>
                     <em> The developed model was able to identify 13 di erent types of plant115</em>
            </p>
            <p>
                     <em>diseases from healthy leaves and was able to distinguish plant leaves from their surroundings. The model was trained116</em>
            </p>
            <p>
                     <em>with 4483(increased to 30,880) images downloaded from the Internet, and the PlantVillage dataset was used to evaluate117</em>
            </p>
            <p>
                     <em>the performance of the proposed technique.</em>
                     <em> Experimental results on the developed model achieved an accuracy of118</em>
            </p>
            <p>
                     <em>between 91% and 98%, with an average of 96.3% for individual class tests.119</em>
            </p>
            <p>
                     <em>In Karthik et al.</em>
                     <em>(2020), Karthik et al. proposed a two-stage deep-learning technique for tomato leaf disease de-120</em>
            </p>
            <p>
                     <em>tection.</em>
                     <em> The ﬁrst architecture applies residual learning to learn essential features of classiﬁcation. The second layer121</em>
            </p>
            <p>
                     <em>architecture applies the attention mechanism to the deep residual network.</em>
                     <em> The experiment was conducted using the122</em>
            </p>
            <p>
                     <em>Plant Village Dataset, which contained three diseases: early blight, late blight, and leaf mold.</em>
                     <em> The author takes advan-123</em>
            </p>
            <p>
                     <em>tage of the features CNN uses attention mechanism to learn in various processing hierarchies, and the overall accuracy124</em>
            </p>
            <p>
                     <em>of the veriﬁcation set reaches 98% in ﬁve-fold cross-validation.125</em>
            </p>
            <p>
                     <em>In Zhang et al.</em>
                     <em>(2020), Zhang et al. proposed an improved fast RCNN to detect healthy tomato leaves and four126</em>
            </p>
            <p>
                     <em>diseases to improve the accuracy of the crop disease leaf recognition model and location of disease leaves. First, the127</em>
            </p>
            <p>
                     <em>author used a deep residual network instead of VGG16 for image feature extraction to obtain deeper disease features.128</em>
            </p>
            <p>
                     <em>Secondly, a k-means clustering algorithm is used to cluster the bounding box, and then anchoring is improved according129</em>
            </p>
            <p>
                     <em>to the clustering results.</em>
                     <em> The improved anchoring framework is the genuine bounding box of the data set. Finally, the130</em>
            </p>
            <p>
                     <em>author conducts a k-means experiment with three feature extraction networks.</em>
                     <em> The experimental results show that the131</em>
            </p>
            <p>
                     <em>improved method is 2.71% more accurate than the original fast RCNN, and the detection speed is faster.132</em>
            </p>
            <p>
                     <em>Patrick et al.</em>
                     <em> In Wspanialy and Moussa (2020), the authors propose a new computer vision system that can auto-133</em>
            </p>
            <p>
                     <em>matically identify several diseases, detect previously undetected diseases, and estimate the severity of each leaf. The134</em>
            </p>
            <p>
                     <em>model was trained and tested using several modiﬁed versions of nine tomato diseases from the PlantVillage tomato135</em>
            </p>
            <p>
                     <em>dataset and showed how di erent leaf attributes a ect disease detection.136</em>
            </p>
            <p>
                     <em>Kawasaki et al.</em>
                     <em> Kawasaki et al.(2015) trained a three-layer convolutional neural network, which can automat-137</em>
            </p>
            <p>
                     <em>ically acquire features required for classiﬁcation and obtain high classiﬁcation performance to diagnose three types138</em>
            </p>
            <p>
                     <em>of cucumber diseases on real farm images where the target object has a complex background. Under the four-fold139</em>
            </p>
            <p>
                     <em>cross-validation strategy, the average accuracy of the model achieved 94.9%.140</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 4 of 14</em>
            </p>
            <p>
                     <em>DeChant et al.</em>
                     <em> DeChant et al.(2017) proposed an automatic system consisting of several layers of convolutional141</em>
            </p>
            <p>
                     <em>neural networks (CNN) for identifying large spot blight lesions on images obtained from maize plant ﬁelds and achiev-142</em>
            </p>
            <p>
                     <em>ing an accuracy of 96.7% on the test set.143</em>
            </p>
            <p>
                     <em>The above studies obtained high judgment accuracy through various convolutional neural networks, but they were144</em>
            </p>
            <p>
                     <em>all based on standardized images with transparent backgrounds.</em>
                     <em> Once the background was blurred, the accuracy would145</em>
            </p>
            <p>
                     <em>be signiﬁcantly reduced, which could not meet the requirements.146</em>
            </p>
            <p>
                     <em>In Zhonghua et al.</em>
                     <em>(2021), Ye Zhonghua et al. studied the real agricultural production environment and ﬁnally147</em>
            </p>
            <p>
                     <em>adopted the SSD target detection model through the comparison and improvement of di erent models to realize the148</em>
            </p>
            <p>
                     <em>prediction of crop image disease regions with complex backgrounds.</em>
                     <em> The experimental results showed that the average149</em>
            </p>
            <p>
                     <em>accuracy of the ﬁnal model in the test set reached 69.894%.</em>
                     <em>150
3. Dataset and method151</em>
            </p>
            <p>
                     <em>3.1. Dataset152</em>
            </p>
            <p>
                     <em>Most of the data sets used in previous studies are from the public data set PlantVillage, which has standard image153</em>
            </p>
            <p>
                     <em>speciﬁcations, simple and clear background, and accurate shooting details.</em>
                     <em> However, the simple background patho-154</em>
            </p>
            <p>
                     <em>logical judgment does not apply to agricultural life.155</em>
            </p>
            <p>
                     <em>In this paper, we used the crawler program written to collect a large number of cucumber leaf image data in batches156</em>
            </p>
            <p>
                     <em>on an agricultural website, which means that these images come from all over China, and most of these images are157</em>
            </p>
            <p>
                     <em>randomly taken by farmers with mobile terminal devices.</em>
                     <em> In real life, most farmers use mobile phones to shoot, so158</em>
            </p>
            <p>
                     <em>there is no suitable equipment to shoot photos with high enough resolution.</em>
                     <em> Moreover, due to the di erent models and159</em>
            </p>
            <p>
                     <em>speciﬁcations of mobile phones, the size and resolution of the images are also di erent, which requires us to process160</em>
            </p>
            <p>
                     <em>them further.</em>
                     <em> Moreover, sample images will be shot directly on farmland without destroying crops, so the background161</em>
            </p>
            <p>
                     <em>of images is complex and changeable, and the shooting angles are diverse, as shown in Figure1.162</em>
            </p>
            <p>
                     <em>With the help of plant pathologists, these images were labeled and became the data set for the experiment. The163</em>
            </p>
            <p>
                     <em>data set consisted of 2392 images, of which 80% were used for the training set and 598 images, or 20%, were used for164</em>
            </p>
            <p>
                     <em>the test set.</em>
                     <em> As shown in the ﬁgure, we propose a lightweight and fast MobileNetv3 network structure that can quickly165</em>
            </p>
            <p>
                     <em>and accurately complete the classiﬁcation of seven kinds of cucumber leaf diseases.</em>
                     <em> The seven pathologic conditions166</em>
            </p>
            <p>
                     <em>are downy mildew, powdery mildew, bacterial angular leaf spot, target leaf spot, gummy stem blight, fusarium wilt,167</em>
            </p>
            <p>
                     <em>and anthracnose.</em>
                     <em> Therefore, the machine vision system proposed in this paper for cucumber pathological diagnosis168</em>
            </p>
            <p>
                     <em>consists of three steps: image acquisition, preprocessing and classiﬁcation, and network model optimization.shown in Figure2.170</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 5 of 14</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 6 of 14
3.2. MobileNet v3171</em>
            </p>
            <p>
                     <em>MobileNetV3 is also a lightweight network.</em>
                     <em> MobileNetV3 uses a network architecture search (NAS) to search the172</em>
            </p>
            <p>
                     <em>global network structure by optimizing each network block, supplemented by the NetAdapt algorithm. This technique173</em>
            </p>
            <p>
                     <em>can e ciently determine an optimal model for a given hardware platform.</em>
                     <em> In addition, MobileNetV3 uses the h-swish174</em>
            </p>
            <p>
                     <em>activation function to improve accuracy Howard et al.(2019)175</em>
            </p>
            <p>
                     <em>In contrast to other classiﬁcation models, it operates a single convolution at each depth of the input image rather176</em>
            </p>
            <p>
                     <em>than combining and ﬂattening all the depths of the input, which is achieved by depth-oriented separable convolution.177</em>
            </p>
            <p>
                     <em>This deep convolution divides the convolution process into two layers, one for ﬁltering and one for merging. This178</em>
            </p>
            <p>
                     <em>combination reduces the size of the model.</em>
                     <em> MobileNetv3 consists of 42D convolution layers,2(112x122) bottleneck179</em>
            </p>
            <p>
                     <em>layers,2(56◊56) bottleneck layers,3(28◊28) bottleneck layers,7(14◊14) bottleneck layers, and 2(7◊7) bottleneck180</em>
            </p>
            <p>
                     <em>layers, in which Swish and Relu are used for activation.</em>
                     <em> Use a pooling layer (7x7) before two dense layers. Extrusion181</em>
            </p>
            <p>
                     <em>and excitation layers are also included, making it faster and lighter.</em>
                     <em> This addition assigns unequal weights to channels182</em>
            </p>
            <p>
                     <em>when creating a map of output elements.</em>
                     <em> Finally, a dense layer with 1024 units is applied to obtain the feature vector.183</em>
            </p>
            <p>
                     <em>The following Table1 is the network structure diagram of MobileNet v3 large.</em>
                     <em> Input in the table is the size of the input184</em>
            </p>
            <p>
                     <em>image.</em>
                     <em> The operator is the convolution layer or the reciprocal residual structure, Exp size and Out are the numbers of185</em>
            </p>
            <p>
                     <em>convolution kernels of the ﬁrst and last layer of the reciprocal residual structure, respectively, and SE is whether the186</em>
            </p>
            <p>
                     <em>SE module is used.</em>
                     <em> NL is the activation function used in the ﬁrst and second layers of the reciprocal residual structure,187</em>
            </p>
            <p>
                     <em>and S is the step size of the deep convolution layer of the reciprocal residual structure.188
3.3. Flooding189</em>
            </p>
            <p>
                     <em>In this paper, the superiority of the network model is judged mainly by the loss size.</em>
                     <em> Firstly, the generation modeof the loss function is introduced.</em>
                     <em> This paper’s experiment’s loss function adopts the cross entropy loss function toclassify the pathology of cucumber leaves into seven categories: C =7 and batch capacity N =12. The calculation</em>
            </p>
            <p>
                     <em>formula of the loss function is as follows.1:</em>
            </p>
            <p>
                     <em>l(p, q)= L =</em>
            </p>
            <p>
                     <em>l1,, lN</em>
            </p>
            <p>
                     <em>Ò, lm =*</em>
            </p>
            <p>
                     <em>C</em>
            </p>
            <p>
                     <em>c=1</em>
            </p>
            <p>
                     <em>wc log</em>
            </p>
            <p>
                     <em>exp</em>
            </p>
            <p>
                     <em>xm,c</em>
            </p>
            <p>
                     <em>≥C</em>
            </p>
            <p>
                     <em>i=1 exp</em>
            </p>
            <p>
                     <em>xm,i</em>
            </p>
            <p>
                     <em>ym,c (1)</em>
            </p>
            <p>
                     <em>Where x is the input,y is the target, w is the weight, and l is the loss function value.190</em>
            </p>
            <p>
                     <em>The loss value of each data sample is calculated through the cross-entropy Loss function.</em>
                     <em> Then the total lossfunction of an epoch is added and calculated according to the batch size to obtain the loss value of the image in the</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 7 of 14
Table 1</em>
            </p>
            <p>
                     <em>MobileNet_v3_large network structure</em>
            </p>
            <p>
                     <em>Input Operator Exp size Out SE NL S224ù224ù3 conv2d ù16ù h-swish 2</em>
            </p>
            <p>
                     <em>112ù112ù16 bneck,3ù31616ù relu 1</em>
            </p>
            <p>
                     <em>112ù112ù16 bneck,3ù36424ù relu 2</em>
            </p>
            <p>
                     <em>56ù56ù24 bneck,3ù37224ù relu 1
56ù56ù24 bneck,5ù57240</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>relu 2</em>
            </p>
            <p>
                     <em>28ù28ù40 bneck,5ù512040</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>relu 1</em>
            </p>
            <p>
                     <em>28ù28ù40 bneck,5ù512040</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>relu 1</em>
            </p>
            <p>
                     <em>28ù28ù40 bneck,3ù324080ù h-swish 2</em>
            </p>
            <p>
                     <em>14ù14ù80 bneck,3ù320080ù h-swish 1</em>
            </p>
            <p>
                     <em>14ù14ù80 bneck,3ù318480ù h-swish 1</em>
            </p>
            <p>
                     <em>14ù14ù80 bneck,3ù318480ù h-swish 1</em>
            </p>
            <p>
                     <em>14ù14ù80 bneck,3ù3480112ù h-swish 1
14ù14ù112 bneck,3ù3672112</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>h-swish 1</em>
            </p>
            <p>
                     <em>14ù14ù112 bneck,5ù5672160</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>h-swish 2</em>
            </p>
            <p>
                     <em>7ù7ù160 bneck,5ù5960160</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>h-swish 1</em>
            </p>
            <p>
                     <em>7ù7ù160 bneck,5ù5960160</em>
            </p>
            <p>
                     <em>˘</em>
            </p>
            <p>
                     <em>h-swish 1</em>
            </p>
            <p>
                     <em>7ù7ù160 conv2d,1ù1ù960ù h-swish 1
7ù7ù960 pool,7ù7ùùùù1</em>
            </p>
            <p>
                     <em>1ù1ù960 conv2d 1ù1, NBN ù1280ù h-swish 1</em>
            </p>
            <p>
                     <em>1ù1ù1280 conv2d 1ù1, NBN ù k ùù1</em>
            </p>
            <p>
                     <em>experiment in Chapter 4. That is,2:</em>
            </p>
            <p>
                     <em>l(p, q)=</em>
            </p>
            <p>
                     <em>N</em>
            </p>
            <p>
                     <em>m=1</em>
            </p>
            <p>
                     <em>lm (2)</em>
            </p>
            <p>
                     <em>In this paper, the size of the loss function is taken as the benchmark for the superiority of the network model. In the191</em>
            </p>
            <p>
                     <em>follow-up experiments, we will ﬁnd that the network model we used has an overﬁtting phenomenon. A loss evaluation192</em>
            </p>
            <p>
                     <em>strategy needs to be replaced.</em>
                     <em> After reaching a certain threshold, the strategy does not take simple loss decline as the193</em>
            </p>
            <p>
                     <em>training orientation.</em>
                     <em> In this way, the loss on the test set shows a relatively ﬂat trend, and then the rising speed of the194</em>
            </p>
            <p>
                     <em>loss on the test set will be reduced, and even a secondary decline may occur.</em>
                     <em> Finally, the accuracy is further improved195</em>
            </p>
            <p>
                     <em>to some extent.</em>
                     <em> We needed a way to solve this problem, and the ﬂooding method came into being. Ishida et al.(2020)196</em>
            </p>
            <p>
                     <em>Consider input variable p À Jd and output variable q À[C]:={1,,C}, where C is the number of classes. They</em>
            </p>
            <p>
                     <em>follow an unknown joint probability distribution with density p(p, q).</em>
                     <em> We denote the score function by f : Jd ô JC</em>
            </p>
            <p>
                     <em>. For any test data point p0, our prediction of the output label will be given by öq0:= argmaxzÀ[C] fz 
</em>
            </p>
            <p>
                     <em>p0</em>
            </p>
            <p>
                     <em>, where</em>
            </p>
            <p>
                     <em>fz( ) is the z -th element of f ( ), and in case of a tie, arg max returns the largest argument. Let l : JC ù[C]ô J</em>
            </p>
            <p>
                     <em>denote a loss function. l can be the zero-one loss, where w :=</em>
            </p>
            <p>
                     <em>w1,,wC</em>
            </p>
            <p>
                     <em>ÒÀ JC , or a surrogate loss such as the 
softmax cross-entropy loss,3:</em>
            </p>
            <p>
                     <em>lCE</em>
            </p>
            <p>
                     <em>w, z®</em>
            </p>
            <p>
                     <em>:=* log</em>
            </p>
            <p>
                     <em>exp</em>
            </p>
            <p>
                     <em>wz®</em>
            </p>
            <p>
                     <em>≥</em>
            </p>
            <p>
                     <em>zÀ[C] exp</em>
            </p>
            <p>
                     <em>wz</em>
            </p>
            <p>
                     <em>.(3)</em>
            </p>
            <p>
                     <em>For a surrogate loss l , we denote the classiﬁcation risk.</em>
                     <em> The goal of multi-class classiﬁcation is to learn f thatminimizes the classiﬁcation error J01(f ).</em>
                     <em> In optimization, we consider the minimization of the risk with a almost</em>
            </p>
            <p>
                     <em>surely di erentiable surrogate loss J (f ) instead to make the problem more tractable.</em>
                     <em> Furthermore, since p(p, q) isusually unknown and there is no way to exactly evaluate J (f ), we minimize its empirical version calculated from the 
training data instead4:</em>
            </p>
            <p>
                     <em>öJ (f ):=1</em>
            </p>
            <p>
                     <em>m</em>
            </p>
            <p>
                     <em>m</em>
            </p>
            <p>
                     <em>i=1</em>
            </p>
            <p>
                     <em>l</em>
            </p>
            <p>
                     <em>f</em>
            </p>
            <p>
                     <em>pi</em>
            </p>
            <p>
                     <em>, qi</em>
            </p>
            <p>
                     <em>(4)</em>
            </p>
            <p>
                     <em>where</em>
            </p>
            <p>
                     <em>pi, qi</em>
            </p>
            <p>
                     <em>m</em>
            </p>
            <p>
                     <em>i=1 are i.i.d. sampled from p(p, q). We call öJ the empirical risk.197</em>
            </p>
            <p>
                     <em>Definition1. The ﬂooded empirical risk is deﬁned as 4
õJ (f )=öJ (f )* b+ b (5)</em>
            </p>
            <p>
                     <em>Note that when b =0, then õJ (f )=öJ (f ).</em>
                     <em> The gradient of õJ (f )w.r.t. model parameters will point to the same direction198</em>
            </p>
            <p>
                     <em>as that of öJ (f ) when öJ (f )> b but in the opposite direction when öJ (f )< b .</em>
                     <em> This means that when the learning199</em>
            </p>
            <p>
                     <em>objective is above the ﬂood level, we perform gradient descent as usual (gravity zone), but when the learning objective200</em>
            </p>
            <p>
                     <em>is below the ﬂood level, we perform gradient ascent instead (buoyancy zone).</em>
                     <em>Pushing the parameters towards a more201</em>
            </p>
            <p>
                     <em>stable region keeps the convergence of the loss function near a threshold value, which improves the generalization202</em>
            </p>
            <p>
                     <em>performance and better resists perturbations.203
4. Experiment204</em>
            </p>
            <p>
                     <em>In this experiment, the image was preprocessed ﬁrst, and the PyTorch framework was used to scale the image to205</em>
            </p>
            <p>
                     <em>448ù448 for data standardization.</em>
                     <em> In this paper, Mobilenet v3 network model was selected as well as optimizer ASGD,206</em>
            </p>
            <p>
                     <em>the learning rate was set to 0.001, the L1 regularity coe cients were all 0.01, the batch size was 12, and 300 rounds207</em>
            </p>
            <p>
                     <em>of iterative training were conducted on the training set and the test set respectively.</em>
                     <em> In order to prevent overﬁtting in208</em>
            </p>
            <p>
                     <em>the experiment, we also apply the algorithm of Dropout to randomly inactivate the neural nodes in the network before209</em>
            </p>
            <p>
                     <em>network training, reduce the interdependence between neurons, and thus ensure the extraction of important features210</em>
            </p>
            <p>
                     <em>that are independent of each other and improve the generalization ability of the model.</em>
                     <em> As shown in the ﬁgure, it can211</em>
            </p>
            <p>
                     <em>be seen that neurons randomly deactivate seven neural nodes in the network.212</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 8 of 14
4.1. Contrast test213</em>
            </p>
            <p>
                     <em>We selected seven mainstream network models and MobileNet v3 network for a comparison test on the same214</em>
            </p>
            <p>
                     <em>cucumber pathological leaf image data set.</em>
                     <em> The experimental data accurately reﬂected the superiority of MobileNet215
v3 network.216</em>
            </p>
            <p>
                     <em>In this paper, Alexnet, Resnet, VGG, E cientnet v2, E cientnet v3, E cientnet v7, Mobilenet v2, and Mobilenet217</em>
            </p>
            <p>
                     <em>v3 leaf pathological recognition models were trained, and image training set and test set were used to test and compare218</em>
            </p>
            <p>
                     <em>them.</em>
                     <em> This way, the network model performance’s superiority is tested and further optimized. The ﬁgure4 shows the219</em>
            </p>
            <p>
                     <em>experimental results, in which vg19 represents VGG model, alex represents Alexnet model, re50 represents Resnet220</em>
            </p>
            <p>
                     <em>model, mob3 represents Mobilenet v3 model, mob2 represents mobilenet v2 model.</em>
                     <em> e 7 represents the E cientnet v7221</em>
            </p>
            <p>
                     <em>model, e 3 represents the E cientnet v3 model, and e 2 represents the E cientnet v2 model.222</em>
            </p>
            <p>
                     <em>The loss function and accuracy of the training set of Alexnet model converge well.</em>
                     <em> When epoch=100, they begin223</em>
            </p>
            <p>
                     <em>to converge and gradually become stable.</em>
                     <em> However, the e ect on the test set could be better, loss and accuracy ﬂuctuate224</em>
            </p>
            <p>
                     <em>considerably, and the convergence e ect could be better.</em>
                     <em> Compared with other models, its loss in the test set is higher,225</em>
            </p>
            <p>
                     <em>its accuracy is lowest, and its performance is poor.226</em>
            </p>
            <p>
                     <em>The training set of the VGG model converges quickly, and the loss image of the data set begins to converge in227</em>
            </p>
            <p>
                     <em>the 70th round of iteration.</em>
                     <em> The accuracy image of the training set and the test set converge faster and in about 30228</em>
            </p>
            <p>
                     <em>iterations.</em>
                     <em> However, the degree of ﬁtting in the test set is not high, and the test set loss and accuracy image of the VGG229</em>
            </p>
            <p>
                     <em>model show no convergence trend. The average accuracy is less than 70%.230</em>
            </p>
            <p>
                     <em>The training set of Resnet begins to converge when the number of training rounds is around 40, and the convergence231</em>
            </p>
            <p>
                     <em>speed is breakneck.</em>
                     <em> When testing the test set, we found that the convergence ﬁtting degree is considerably high, and232</em>
            </p>
            <p>
                     <em>the maximum accuracy is as high as 81.4%.</em>
                     <em> However, the ﬂuctuation range of loss and accuracy of the test set is larger233</em>
            </p>
            <p>
                     <em>than that of Alexnet network model numerically, and the loss function also appears to be an overﬁtting phenomenon.234</em>
            </p>
            <p>
                     <em>The results of E cientnet v2 and E cientnet v7 models are the same.</em>
                     <em> The loss and accuracy of the test set tend235</em>
            </p>
            <p>
                     <em>to converge, and the ﬁtting degree is higher than that of the training set.</em>
                     <em> However, the degree of overﬁtting of the loss236</em>
            </p>
            <p>
                     <em>image of the test set is too high and ﬂuctuates wildly.237</em>
            </p>
            <p>
                     <em>The E cientnet v3 model, where the precision image of the test set begins to converge around the 150th iteration238</em>
            </p>
            <p>
                     <em>round, is the slowest of all models.</em>
                     <em> The test set of E cientnet v3 shows a convergence e ect, but a severe overﬁtting239</em>
            </p>
            <p>
                     <em>phenomenon occurs, and the ﬂuctuation is the largest from the experimental results.240</em>
            </p>
            <p>
                     <em>In the MobileNet v2 model, the training set starts to converge from the 30th iteration, the loss ﬁnally keeps ap-241</em>
            </p>
            <p>
                     <em>proaching 0, and the accuracy also keeps increasing with the training rounds.</em>
                     <em> The resulting trend of the test set also242</em>
            </p>
            <p>
                     <em>roughly ﬁts the training set, but there is an overﬁtting phenomenon.</em>
                     <em> The generalization degree is shallow, and the243
</em>
                     <em>ﬂuctuation degree is enormous.244</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 9 of 14</em>
            </p>
            <p>
                     <em>Considering the ﬁtting e ect of each model test set comprehensively, the test set result trend of Mobilenet v3245</em>
            </p>
            <p>
                     <em>network is consistent with the training set trend, and its maximum accuracy is relatively the highest, reaching 81.3% or246</em>
            </p>
            <p>
                     <em>above.</em>
                     <em> Moreover, Mobilenet v3 converges 66% faster than other networks due to its lightweight framework. Therefore,247</em>
            </p>
            <p>
                     <em>we ﬁnally chose Mobilenet v3 network for the next optimization experiment.</em>
                     <em>248
4.2. The choice of optimizer249</em>
            </p>
            <p>
                     <em>After selecting Mobilenet v3 as the ﬁnal experimental network, this paper optimizes it.</em>
                     <em> The ﬁrst is the selection of250</em>
            </p>
            <p>
                     <em>the optimizer.</em>
                     <em> The optimizer is used to update and calculate network parameters that a ect model training and model251</em>
            </p>
            <p>
                     <em>output to approximate or reach the optimal value, thereby minimizing (or maximizing) the loss function. Choosing252</em>
            </p>
            <p>
                     <em>an appropriate optimizer can make our network model reach convergence faster and achieve better accuracy. On the253</em>
            </p>
            <p>
                     <em>same cucumber pathological leaf image data set, Mobilenet v3 network was selected in this paper, and the learning254</em>
            </p>
            <p>
                     <em>rate was set to 0.001, the regularity coe cients of L1 and L2 were both 0.01, and 300 rounds of iterative training255</em>
            </p>
            <p>
                     <em>were conducted on the training set and test set respectively.</em>
                     <em> In this paper, ASGD, SGD, RMSprop, RAdam, NAdam,256</em>
            </p>
            <p>
                     <em>AdamW, Adamax, Adadelta, and Agagrad are selected for comparison, and loss functions and accurate images of the257</em>
            </p>
            <p>
                     <em>training set and test set are obtained, as shown in the ﬁgure.5258</em>
            </p>
            <p>
                     <em>As shown in the ﬁgure, although the four optimizers, RAdam, NAdam, Adam, and RMSprop, all have a conver-259</em>
            </p>
            <p>
                     <em>gence trend at last, their loss value on the training set is very high, and their highest accuracy is not more than 60%.260</em>
            </p>
            <p>
                     <em>Compared with other optimizers, the e ect could be better, and they are unsuitable for this paper’s network model.261</em>
            </p>
            <p>
                     <em>AdamW optimizer performs well in the training set.</em>
                     <em> The convergence rate of the loss function and accuracy image262</em>
            </p>
            <p>
                     <em>is the fastest compared with other optimizers.</em>
                     <em> However, its performance in the test set could be better. The loss function263</em>
            </p>
            <p>
                     <em>and accuracy image have large ﬂuctuations, and its maximum accuracy is at most 70%.264</em>
            </p>
            <p>
                     <em>Adamax optimizer begins to converge after 100 iteration rounds of the training set.</em>
                     <em> Its loss function image value265</em>
            </p>
            <p>
                     <em>in the test set is higher than that of the ten optimizers, and its accuracy is low, with an average accuracy of less than266
65%.267</em>
            </p>
            <p>
                     <em>The data set images of Adadelta and Adagrad optimizers almost coincide.</em>
                     <em> Both the training set and the test set268</em>
            </p>
            <p>
                     <em>converge.</em>
                     <em> The loss value gradually decreases with the increase in the number of iterations, which is the lowest among269</em>
            </p>
            <p>
                     <em>the ten optimizers.</em>
                     <em> The accuracy also increases with the number of iterations, reaching a high accuracy of 83%.270</em>
            </p>
            <p>
                     <em>However, its convergence speed could be faster.</em>
                     <em> The training set begins to converge in the 250th iteration round, and271</em>
            </p>
            <p>
                     <em>the test set begins to converge in the 150th iteration round, which takes the longest time.272</em>
            </p>
            <p>
                     <em>The data set images of the two optimizers, ASGD and SGD, almost coincide, converge in both the training set and273</em>
            </p>
            <p>
                     <em>the test set, and the convergence is faster.</em>
                     <em> The training set begins to converge in the 80th iteration round, and the test274</em>
            </p>
            <p>
                     <em>set begins to converge in the 20th iteration.</em>
                     <em> The accuracy of the test set peaked at 81.4%. Numerically, the ASGD has275</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 10 of 14
Table 2</em>
            </p>
            <p>
                     <em>Use eﬀect of diﬀerent batch sizes</em>
            </p>
            <p>
                     <em>batch_size max_acc(%) mean_acc(%) sstd_acc(%) max_loss mean_loss std_loss</em>
            </p>
            <p>
                     <em>479.87064.19710.9161367.639367.173181.912</em>
            </p>
            <p>
                     <em>881.79475.4923.227178.613125.19620.972</em>
            </p>
            <p>
                     <em>1282.11576.5682.962135.26176.6199.009</em>
            </p>
            <p>
                     <em>1681.54276.8483.556123.97655.9406.853</em>
            </p>
            <p>
                     <em>2082.45176.6264.069118.69944.2326.210</em>
            </p>
            <p>
                     <em>2481.75176.8544.498117.16636.6385.916</em>
            </p>
            <p>
                     <em>2882.24576.5275.140113.50231.2515.801</em>
            </p>
            <p>
                     <em>3282.23076.4865.303112.95827.3935.897</em>
            </p>
            <p>
                     <em>less ﬂuctuation than the SGD optimizer.276</em>
            </p>
            <p>
                     <em>Therefore, after comparing convergence speed,ﬁtting degree, accuracy, loss function size, and other aspects,277</em>
            </p>
            <p>
                     <em>ASGD has a higher peak value, faster convergence, and minor ﬂuctuation.</em>
                     <em> In this article, the ASGD is chosen as278
the ﬁnal optimizer.279</em>
            </p>
            <p>
                     <em>subsectionThe choice of batch size In this comparison experiment, Mobilenet v3 network model was selected, the280</em>
            </p>
            <p>
                     <em>optimizer was ASGD, the learning rate was set to 0.001, the regularity coe cients of L1 and L2 were both 0.01, and281</em>
            </p>
            <p>
                     <em>300 rounds of iterative training were conducted on the training set and test set respectively.</em>
                     <em> Batch size was selected as282</em>
            </p>
            <p>
                     <em>4,8,12,16,20,24,28, and 32. The images and data results were obtained as shown in the ﬁgure below.6283</em>
            </p>
            <p>
                     <em>Batch size =4, as the batch size value is too small, the gradient of each layer has high randomness and takes much284</em>
            </p>
            <p>
                     <em>time.</em>
                     <em> The resulting image also ﬂuctuates, and the ﬁnal precision e ect is considerably poor, resulting in an underﬁtting285</em>
            </p>
            <p>
                     <em>phenomenon. The convergence e ect is not good enough.286</em>
            </p>
            <p>
                     <em>It can be seen from the ﬁgure6 that the convergence speed increases with the increase in batch size. According287</em>
            </p>
            <p>
                     <em>to the numerical results,2 with the increase of batch size, the maximum loss function, average loss function, and the288</em>
            </p>
            <p>
                     <em>standard deviation of the loss function, namely the volatility, of the test set gradually decrease. However, after batch289</em>
            </p>
            <p>
                     <em>size =12, each loss value changes little with the increase in batch size.</em>
                     <em> In addition, the maximum accuracy after290</em>
            </p>
            <p>
                     <em>convergence increases weakly and sometimes even regresses.</em>
                     <em> Moreover, after batch size =12, the standard deviation291</em>
            </p>
            <p>
                     <em>of the accuracy of the test set began to rise continuously, indicating that the model’s generalization ability declined.292</em>
            </p>
            <p>
                     <em>Before batch size =12, the test set’s accuracy increases while the loss ﬂuctuation decreases. When batch size =12,293</em>
            </p>
            <p>
                     <em>the standard deviation of loss is minimum, and the anti-aliasing e ect is best.</em>
                     <em> At the same time, the accuracy of the294
</em>
                     <em>test set increased to 82.1%.295</em>
            </p>
            <p>
                     <em>The experiment in this paper is carried out under a blurred background image, so we need as much generalization296</em>
            </p>
            <p>
                     <em>ability as possible.</em>
                     <em> Moreover, the model proposed in this paper should apply to mobile terminal devices, should be as297</em>
            </p>
            <p>
                     <em>lightweight as possible, and need to select the smallest batch size value possible.</em>
                     <em> Therefore, from the perspective of298</em>
            </p>
            <p>
                     <em>background requirements and image data analysis, batch size =12 was selected as the optimal experimental parameter299</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 11 of 14
Table 3</em>
            </p>
            <p>
                     <em>Use eﬀect after diﬀerent values of parameter b in ﬂooding</em>
            </p>
            <p>
                     <em>b max_acc(%) mean_acc(%) sstd_acc(%) max_loss mean_loss std_loss0.34981.64476.0252.980139.20763.7065.395</em>
            </p>
            <p>
                     <em>0.29181.60876.5083.076136.61065.3885.288</em>
            </p>
            <p>
                     <em>0.25282.06876.3443.080138.87967.1565.405</em>
            </p>
            <p>
                     <em>0.31183.30878.2632.658135.18764.8815.256</em>
            </p>
            <p>
                     <em>0.29781.64176.3152.980139.06565.0605.400</em>
            </p>
            <p>
                     <em>0.33080.89076.3953.248138.47564.6705.387</em>
            </p>
            <p>
                     <em>0.29781.37576.4582.953138.85864.9295.319</em>
            </p>
            <p>
                     <em>0.29681.56876.3853.146137.58065.5045.320</em>
            </p>
            <p>
                     <em>0.27482.55776.4973.249136.90965.8205.494</em>
            </p>
            <p>
                     <em>0.17182.27176.7173.136136.26969.9506.173</em>
            </p>
            <p>
                     <em>0.23282.82076.6103.306136.02167.7365.490</em>
            </p>
            <p>
                     <em>0.26581.78676.7023.238134.72766.2505.396</em>
            </p>
            <p>
                     <em>0.25681.82876.2173.118138.51066.8545.473</em>
            </p>
            <p>
                     <em>0.20781.63476.4963.263137.23769.3575.813</em>
            </p>
            <p>
                     <em>0.22382.58876.7823.382136.99567.8195.571
in this paper.300</em>
            </p>
            <p>
                     <em>4.3. Flooding301</em>
            </p>
            <p>
                     <em>In Chapter 3, we introduced the basic principle and used a ﬂooding mode.</em>
                     <em> By changing the loss function and302</em>
            </p>
            <p>
                     <em>adding a threshold, the loss eventually ﬂuctuated around the threshold.</em>
                     <em> Flooding allows us to directly select the level303</em>
            </p>
            <p>
                     <em>of training loss, which is di cult to achieve with other regularizers.</em>
                     <em> There was an overﬁtting phenomenon in the loss304</em>
            </p>
            <p>
                     <em>result in images of the Mobilenet v3 experiment mentioned above.</em>
                     <em> In this section,ﬂooding was used to realize the305</em>
            </p>
            <p>
                     <em>secondary decrease of data set loss and prevent overﬁtting.306</em>
            </p>
            <p>
                     <em>In this experiment, the optimizer used ASGD, the learning rate was 0.001, the regularization coe cients of L1307</em>
            </p>
            <p>
                     <em>and L2 were 0.01, and 300 iteration experiments were conducted.</em>
                     <em> The loss threshold is set with 15 di erent values for308</em>
            </p>
            <p>
                     <em>comparative analysis of images and data.309</em>
            </p>
            <p>
                     <em>As seen from the image,7 after ﬂooding was added, the overﬁtting rising trend of the loss function image in the test310</em>
            </p>
            <p>
                     <em>set was e ectively suppressed.</em>
                     <em> When b =0.310,0.348, and 0.290, the ﬂooding not only resulted in good inhibition311</em>
            </p>
            <p>
                     <em>but also resulted in secondary descending, which solved the overﬁtting problem.312</em>
            </p>
            <p>
                     <em>According to a series of comparisons of table data, after adding ﬂooding to 3, the mean test set accuracy increased313</em>
            </p>
            <p>
                     <em>by 0.2%, and the maximum test set accuracy increased by 0.5%.</em>
                     <em> The ﬁnal goal of this paper is to select the test set314</em>
            </p>
            <p>
                     <em>with the highest accuracy to achieve the best pathological recognition e ect of the cucumber leaf image. The ﬁnal315</em>
            </p>
            <p>
                     <em>selection threshold is 0.310, at which time the overﬁtting of the loss function is well suppressed, and the accuracy is316
317</em>
            </p>
            <p>
                     <em>We compared the two experiments without ﬂooding with the ﬂooding method.</em>
                     <em> The results are shown in the ﬁgure.8318</em>
            </p>
            <p>
                     <em>Methods with ﬂooding tend to improve test accuracy compared to baseline methods without ﬂooding. Continue to train319</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 12 of 14</em>
            </p>
            <p>
                     <em>the model without ﬂooding until, eventually, the loss function may continue to rise and accuracy may decline. However,320</em>
            </p>
            <p>
                     <em>according to the results, the ﬁnal model has good predictive performance when there is ﬂooding, which means that321</em>
            </p>
            <p>
                     <em>ﬂooding helps improve test accuracy in later training.</em>
                     <em> During training with ﬂooding, test losses became lower and322</em>
            </p>
            <p>
                     <em>ﬂatter.</em>
                     <em> On the other hand, the training loss reached a secondary decline and continued to ﬂoat around the ﬂooding323
threshold with stability.324</em>
            </p>
            <p>
                     <em>4.4. Discussion325</em>
            </p>
            <p>
                     <em>In this study, the data set was replaced with PlantVillage public data set and another public apple disease data set326</em>
            </p>
            <p>
                     <em>in China to conduct the pathological judgment experiment of apple leaves.</em>
                     <em> In this experiment,10 rounds of iterative327</em>
            </p>
            <p>
                     <em>experiments were conducted.</em>
                     <em> The experimental results are shown in the ﬁgure below, where the apple curve represents328</em>
            </p>
            <p>
                     <em>the experimental results using the apple disease data set.</em>
                     <em> The plant curve represented the experimental results using329</em>
            </p>
            <p>
                     <em>the PlantVillage public data set.330</em>
            </p>
            <p>
                     <em>As can be seen from the image results,9 the loss function of the training set and the test set is constantly close to331</em>
            </p>
            <p>
                     <em>0, and the accuracy also increases with the increase of iteration rounds.</em>
                     <em> The accuracy of PlantVillage public data set332</em>
            </p>
            <p>
                     <em>after applying the strategy in this paper is as high as 99%, and the accuracy of the apple disease data set is also as high333</em>
            </p>
            <p>
                     <em>as 98.1%, which is far higher than the 76.5% accuracy of Zhou Minmin’s apple-leaf-disease-detection-system based334</em>
            </p>
            <p>
                     <em>on transfer learning.</em>
                     <em>Minmin (2019) It is proved that compared with the existing strategies, the proposed strategies are335</em>
            </p>
            <p>
                     <em>universal, accurate, and less time-consuming and can better meet the needs of Chinese farmers for crop pathological336
</em>
                     <em>judgment in today’s society.337</em>
            </p>
            <p>
                     <em>5. Conclusion338</em>
            </p>
            <p>
                     <em>In today’s society, the rise of the online QA system has brought great convenience to people’s lives, but it is not339</em>
            </p>
            <p>
                     <em>widely used in agriculture.</em>
                     <em> The pathological judgment of agricultural plants is an essential part of agricultural planting340</em>
            </p>
            <p>
                     <em>life.</em>
                     <em> Today’s crop pathological judgment mostly requires high labor costs, low detection e ciency, and poor reliability341</em>
            </p>
            <p>
                     <em>because of its dense growing environment and chaotic background.342</em>
            </p>
            <p>
                     <em>To solve these problems, this paper proposed a Mobilenet v3 based on ﬂooding to identify crop leaves in fuzzy343</em>
            </p>
            <p>
                     <em>scenes.</em>
                     <em> It satisﬁed the requirement of mobile terminal using a lightweight framework and could quickly and accurately344</em>
            </p>
            <p>
                     <em>judge crop pathological conditions through farmers’ shooting pictures.345</em>
            </p>
            <p>
                     <em>In this paper, cucumber leaf images were randomly collected from a Chinese agricultural website and labeled.346</em>
            </p>
            <p>
                     <em>A dataset with complex image background was constructed, and seven kinds of cucumber leaf pathologic judgments347</em>
            </p>
            <p>
                     <em>were made.</em>
                     <em> Through the control variable method, the network model, the optimizer, and batch size, three rounds of348</em>
            </p>
            <p>
                     <em>experiments were compared and analyzed to achieve the optimal network model.</em>
                     <em> In this paper,ﬂooding method was349</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 13 of 14</em>
            </p>
            <p>
                     <em>used to replace an evaluation strategy of loss.</em>
                     <em> The accuracy of the test set was increased by 0.5% again, reaching the350</em>
            </p>
            <p>
                     <em>highest 88.3%.</em>
                     <em> Finally, two public data sets of PlantVillage and apple disease were selected for the experiment again.351</em>
            </p>
            <p>
                     <em>The accuracy was up to 99% and 98.1%, respectively, which proved the universality of the proposed strategy and its352
353</em>
            </p>
            <p>
                     <em>References354</em>
            </p>
            <p>
                     <em>Benfenati, A.</em>
                     <em>, Causin, P., Oberti, R., Stefanello, G.,2021. Unsupervised deep learning techniques for powdery mildew recognition based on355</em>
            </p>
            <p>
                     <em>multispectral imaging. arXiv preprint arXiv:2112.11242.356</em>
            </p>
            <p>
                     <em>Cap, Q.</em>
                     <em>H., Uga, H., Kagiwada, S., Iyatomi, H.,2020. Leafgan: An e ective data augmentation method for practical plant disease diagnosis. IEEE357</em>
            </p>
            <p>
                     <em>Transactions on Automation Science and Engineering .358</em>
            </p>
            <p>
                     <em>DeChant, C., Wiesner-Hanks, T.</em>
                     <em>, Chen, S., Stewart, E.L., Yosinski, J., Gore, M.A., Nelson, R.J., Lipson, H.,2017. Automated identiﬁcation of359</em>
            </p>
            <p>
                     <em>northern leaf blight-infected maize plants from ﬁeld imagery using deep learning.</em>
                     <em> Phytopathology 107,1426-1432.360</em>
            </p>
            <p>
                     <em>Howard, A.</em>
                     <em>, Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al.,2019. Searching for361</em>
            </p>
            <p>
                     <em>mobilenetv3, in: Proceedings of the IEEE/CVF international conference on computer vision, pp.1314-1324.362</em>
            </p>
            <p>
                     <em>Hughes, D., Salathé, M.</em>
                     <em>, et al.,2015. An open access repository of images on plant health to enable the development of mobile disease diagnostics.363</em>
            </p>
            <p>
                     <em>arXiv preprint arXiv:1511.08060.364</em>
            </p>
            <p>
                     <em>Ishida, T., Yamane, I.</em>
                     <em>, Sakai, T., Niu, G., Sugiyama, M.,2020. Do we need zero training loss after achieving zero training error? arXiv preprint365
arXiv:2002.08709.366</em>
            </p>
            <p>
                     <em>Karthik, R.</em>
                     <em>, Hariharan, M., Anand, S., Mathikshara, P., Johnson, A., Menaka, R.,2020. Attention embedded residual cnn for disease detection in367</em>
            </p>
            <p>
                     <em>tomato leaves. Applied Soft Computing 86,105933.368</em>
            </p>
            <p>
                     <em>Kawasaki, Y., Uga, H.</em>
                     <em>, Kagiwada, S., Iyatomi, H.,2015. Basic study of automated diagnosis of viral plant diseases using convolutional neural369</em>
            </p>
            <p>
                     <em>networks, in: International symposium on visual computing, Springer. pp.638-645.370</em>
            </p>
            <p>
                     <em>Martinelli, F., Scalenghe, R.</em>
                     <em>, Davino, S., Panno, S., Scuderi, G., Ruisi, P., Villa, P., Stroppiana, D., Boschetti, M., Goulart, L.R., et al.,2015.371</em>
            </p>
            <p>
                     <em>Advanced methods of plant disease detection.</em>
                     <em> a review. Agronomy for Sustainable Development 35,1-25.372</em>
            </p>
            <p>
                     <em>Minmin, Z.</em>
                     <em>,2019. Research on Android Detection System of Apple Leaf Disease Based on Transfer Learning.</em>
                     <em> Master’s thesis. Northwest AF373
University.374</em>
            </p>
            <p>
                     <em>Prasanna Mohanty, S., Hughes, D.</em>
                     <em>, Salathe, M.,2016. Using deep learning for image-based plant disease detection. arXiv e-prints , arXiv-1604.375</em>
            </p>
            <p>
                     <em>Saikawa, T., Cap, Q.</em>
                     <em>H., Kagiwada, S., Uga, H., Iyatomi, H.,2019. Aop: An anti-overﬁtting pretreatment for practical image-based plant diagnosis,376</em>
            </p>
            <p>
                     <em>in:2019 IEEE International Conference on Big Data (Big Data), IEEE. pp.5177-5182.377</em>
            </p>
            <p>
                     <em>Sladojevic, S.</em>
                     <em>, Arsenovic, M., Anderla, A., Culibrk, D., Stefanovic, D.,2016. Deep neural networks based recognition of plant diseases by leaf378</em>
            </p>
            <p>
                     <em>image classiﬁcation. Computational intelligence and neuroscience 2016.379</em>
            </p>
            <p>
                     <em>Wspanialy, P.</em>
                     <em>, Moussa, M.,2020. A detection and severity estimation system for generic diseases of tomato greenhouse plants. Computers and380</em>
            </p>
            <p>
                     <em>Electronics in Agriculture 178,105701.381</em>
            </p>
            <p>
                     <em>Zhang, Y., Song, C.</em>
                     <em>, Zhang, D.,2020. Deep learning-based object detection improvement for tomato disease. IEEE Access 8,56607-56614.382</em>
            </p>
            <p>
                     <em>Zhonghua, Y.</em>
                     <em>, Mingxia, Z., Lu, J.,2021. Research on crop disease image recognition with complex background. Transactions of the Chinese383</em>
            </p>
            <p>
                     <em>Society for Agricultural Machinery .384</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 14 of 14</em>
            </p>
            <p>
                     <em>short author name: Preprint submitted to Elsevier Page 15 of 14
List of Figures385</em>
            </p>
            <p>
                     <em>1 Cucumber complex background data set presentation ..........................16386</em>
            </p>
            <p>
                     <em>2 Article algorithm .............................................17387</em>
            </p>
            <p>
                     <em>3 A comparison of the neural network before and after using dropout ...................18388</em>
            </p>
            <p>
                     <em>4 Comparison Experiment between mobilenet v3 and Mainstream Image Classiﬁcation Algorithms ...19389</em>
            </p>
            <p>
                     <em>5 Comparison Experiment between ASGD and Mainstream Optimizer Experiment ...........20390</em>
            </p>
            <p>
                     <em>6 Comparative Experiments with Di erent batch Sizes ..........................21391</em>
            </p>
            <p>
                     <em>7 Comparative Experiment on Di erent Values of Parameter b After Using ﬂooding ...........22392</em>
            </p>
            <p>
                     <em>8 Comparative Experiment Before and After Flooding .</em>
                     <em>..........................23393
9 E ect experiment of mobile v3 based on ﬂooding on di erent data sets .................24394</em>
            </p>
        </div>

     </div>
 </div>
<!--e main -->

</body>
</html>